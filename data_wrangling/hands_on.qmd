---
title: "Data wrangling"
format:
  html: default
  ipynb: default
jupyter: sds
---

You know the basics. What are Jupyter notebooks, how do they work, and how do you run
Python in them. It is time to start using them for data science (no, that simple math
you did the last time doesn't count as data science).

You are about to enter the PyData ecosystem. It means that you will start learning
how to work with Python from the middle. This course does not explicitly cover the
fundamentals of programming. It is expected that those parts you need you'll be able to
pick as you go through the specialised data science stack. If you're stuck,
confused or need further explanation, use Google (or your favourite
search engine), ask AI to explain the code or ask on Discord or during the class. Not
everything will be told during the course (by design), and the internet is a friend of
every programmer, so let's figure out how to use it efficiently from the beginning.

Let's dig in!

## Munging and wrangling

Real-world datasets are messy. There is no way around it: datasets have “holes” (missing
data), the amount of formats in which data can be stored is endless, and the best
structure to share data is not always the optimum to analyse them, hence the need to
[munge](http://dictionary.reference.com/browse/munge)[^1] them. As has been correctly
pointed out in many outlets, much of the time spent in what is called Data Science is
related not only to sophisticated modelling and insight but has to do with much more
basic and less exotic tasks such as obtaining data, processing, and turning them into a
shape that makes analysis possible, and exploring it to get to know their basic
properties.

[^1]: Data munging and data wrangling are used interchangeably. Pick the one you like.

Surprisingly, very little has been published on patterns, techniques, and best practices
for quick and efficient data cleaning, manipulation, and transformation because of how
labour-intensive and relevant this aspect is. In this session, you will use a few
real-world datasets and learn how to process them into Python so they can be transformed
and manipulated, if necessary, and analysed. For this, you will introduce some of the
bread and butter of data analysis and scientific computing in Python. These are
fundamental tools that are constantly used in almost any task relating to data analysis.

This notebook covers the basics and the content that is expected to be learnt by every
student. You use a prepared dataset that saves us much of the more intricate processing
that goes beyond the introductory level the session is aimed at. If you are interested
in how it was done, there is a
[notebook](https://github.com/martinfleis/sds/blob/main/data/chicago_influenza_1918/preprocessing.ipynb).

This notebook discusses several patterns to clean and structure data properly,
including tidying, subsetting, and aggregating. You finish with some basic visualisation.
An additional extension presents more advanced tricks to manipulate tabular data.

## Dataset

You will be exploring demographic characteristics of Chicago in 1918 linked to the
influenza mortality during the pandemic that happened back then, coming from the
research paper by @grantz2016disparities. The data are aggregated to census tracts and
contain information on unemployment, home ownership, age structure and influenza
mortality from a period of 8 weeks.

The main tool you use is the `pandas` package. As with the `math` you used
[before](../introduction/hands_on.qmd), you must import it first.

```{python}
import pandas as pd  # <1>
```
1. Import the `pandas` package under the alias `pd`. Using the alias is not
necessary, but it is a convention nearly everyone follows.

The data is stored in a CSV file. To make things easier, you can read data from a file
posted online so, for now, you do not need to download any dataset:

```{python}
chicago_1918 = pd.read_csv(                                                         # <1>
    "https://martinfleischmann.net/sds/data_wrangling/data/chicago_influenza_1918.csv", # <2>
    index_col="geography_code",                                                     # <3>
)
```
1. Use the `read_csv` function from `pandas`. Remember that you have imported `pandas` as `pd`.
2. Specify the path to the file. It could be a web address like here or a local file path.
3. Use the column `geography_code` as an index of the table by passing its name to the
`index_col` keyword argument. It is not strictly necessary but allows us to choose and index
on reading instead of specifying it later. More on indices below.

::: {.callout-tip}
You are using `read_csv` because the file you want to read is in CSV format. However,
`pandas` allows for many more formats to be read and write. A full list of formats
supported may be found in [the documentation](https://pandas.pydata.org/docs/user_guide/io.html).
:::

::: {.callout-note}
## Alternative
Instead of reading the file directly off the web, it is possible to download it manually,
store it on your computer, and read it locally. To do that, you can follow these steps:

1. Download the file by right-clicking on
[this link](https://martinfleischmann.net/sds/data_wrangling/data/chicago_influenza_1918.csv)
and saving the file
2. Place the file in the same folder as the notebook where you intend to read it
3. Replace the code in the cell above with:

```python
chicago_1918 = pd.read_csv(
    "chicago_influenza_1918.csv",
    index_col="geography_code",
)
```
:::
## Pandas 101

Now, you are ready to start playing and interrogating the dataset! What you have at your
fingertips is a table summarising, for each of the census tracts in Chicago more than
a century ago, how many people lived in each by age, accompanied by some other
socioeconomic data and influenza mortality. These tables are called `DataFrame` objects,
and they have a lot of functionality built-in to explore and manipulate the data they
contain. Let’s explore a few of those cool tricks!

### Data Structures

The first aspect worth spending a bit of time on is the structure of a `DataFrame`. You
can print it by simply typing its name:

```{python}
chicago_1918
```

Note the printing is cut to keep a nice and compact view but enough to see its
structure. Since they represent a table of data, `DataFrame` objects have two dimensions:
rows and columns. Each of these is automatically assigned a name in what you will call
its _index_. When printing, the index of each dimension is rendered in bold, as opposed
to the standard rendering for the content. The example above shows how the
column index is automatically picked up from the `.csv` file’s column names. For rows,
we have specified when reading the file you wanted the column `geography_code`, so that is
used. If you hadn’t set any, `pandas` would automatically generate a sequence starting
in `0` and going all the way to the number of rows minus one. This is the standard
structure of a `DataFrame` object, so you will come to it over and over. Importantly, even
when you move to spatial data, your datasets will have a similar structure.

One final feature that is worth mentioning about these tables is that they can hold
columns with different types of data. In this example, you have
counts (or `int` for integer types) and ratios (or 'float' for floating point numbers
 - a number with decimals) for each column. But it is useful to keep in mind that
you can combine this with columns that hold other types of data such as categories, text
(`str`, for string), dates or, as you will see later in the course, geographic features.

To extract a single column from this `DataFrame`, specify its name
in the square brackets (`[]`). Note that the name, in this case, is a `string`. A piece
of text. As such, it needs to be within single (`'`) or double quotes (`"`). The resulting
data structure is no longer a `DataFrame`, but you have a `Series` because you deal with a
single column.

```{python}
chicago_1918["influenza"]
```

### Inspect

Inspecting what it looks like. You can check the table's top (or bottom) X lines by
passing X to the method `head` (`tail`). For example, for the top/bottom five lines:

```{python}
chicago_1918.head()
```

```{python}
chicago_1918.tail()
```

Or get an overview of the table:

```{python}
chicago_1918.info()
```

### Summarise

Or of the _values_ of the table:

```{python}
chicago_1918.describe()
```

Note how the output is also a `DataFrame` object, so you can do with it the same things
you would with the original table (e.g. writing it to a file).

In this case, the summary might be better presented if the table is “transposed”:

```{python}
chicago_1918.describe().T
```

Equally, common descriptive statistics are also available. To obtain minimum values for
each column, you can use `.min()`.

```{python}
chicago_1918.min()
```

Or to obtain a minimum for a single column only.

```{python}
chicago_1918["influenza"].min()
```

Note here how you have restricted the calculation of the minimum value to one column only
by getting the `Series` and calling `.min()` on that.

Similarly, you can restrict the calculations to a single row using `.loc[]` indexer:

```{python}
chicago_1918.loc["G17003100492"].max()
```

### Create new columns

You can generate new variables by applying operations to existing ones. For example, you
can calculate the total population by area. Here are a couple of ways to do it:

```{python}
# This one is longer, hardcoded
total_population = (            # <1>
    chicago_1918["agecat1"]     # <2>
    + chicago_1918["agecat2"]   # <2>
    + chicago_1918["agecat3"]   # <2>
    + chicago_1918["agecat4"]   # <2>
    + chicago_1918["agecat5"]   # <2>
    + chicago_1918["agecat6"]   # <2>
    + chicago_1918["agecat7"]   # <2>
)
total_population.head()         # <3>
```
1. Create a new variable called `total_population` to store the result.
2. Select all the columns and add them together
3. Print the top of the variable

```{python}
# This one is shorted, using a range of columns and sum
total_population = chicago_1918.loc[:, "agecat1":"agecat7"].sum(axis=1)  # <1>
total_population.head()
```
1. This line is simple, but a lot happens here. Using `.loc[]`, you select all the rows
(`:` part) and all the columns between `"agecat1"` and `"agecat7"`. Then you apply
`.sum()` over `axis=1`, which means along rows, to get a sum per each row.

Once you have created the variable, you can make it part of the table:

```{python}
chicago_1918["total_population"] = total_population  # <1>
chicago_1918.head()
```
1. Assing a variable `total_population` that contains a `Series` as a column
`"total_population"`. `pandas` creates that column automatically. If it existed, it would
get overridden.

You can also do other mathematical operations on columns. These are always automatically
applied to individual values in corresponding rows.

```{python}
homeowners = chicago_1918["total_population"] * chicago_1918["ho_pct"]  # <1>
homeowners.head()
```
1. A product of the total population and home ownership percentage provides an estimation
of the number of homeowners per census tract.

```{python}
pop_density = chicago_1918["total_population"] / chicago_1918["gross_acres"]  # <1>
pop_density.head()
```
1. A division of the total population by the area results in an estimation
of the population density.

A different spin on this is assigning new values: you can generate new variables with
scalars[^scalar], and modify those:

[^scalar]: Scalar is a single value, like a number (`42`) or a string (`"towel"`).

```{python}
chicago_1918["ones"] = 1  # <1>
chicago_1918.head()
```
1. Create a new column named `"ones"` with all ones.

And you can modify specific values too:

```{python}
chicago_1918.loc["G17003100001", "ones"] = 3
chicago_1918.head()
```

### Remove columns

Permanently deleting variables is also within reach of one command:

```{python}
chicago_1918 = chicago_1918.drop(columns="ones")
chicago_1918.head()
```

### Index-based queries

Here, you explore how to subset parts of a `DataFrame` if you know exactly which bits
you want. For example, if you want to extract the influenza mortality and total population of the
first four areas in the table, you use `loc` with lists:

```{python}
death_pop_first4 = chicago_1918.loc[                                   # <1>
    ["G17003100001", "G17003100002", "G17003100003", "G17003100004"],  # <2>
    ["influenza", "total_population"],                                 # <3>
]
death_pop_first4
```
1. `loc` takes two inputs. Selection of rows and selection of columns. If the latter is
not present, it is assumed that all the columns are selected. The same could be achieved by using `:`.
2. A `list` of index values. Note that you use squared brackets (`[]`) to delineate the
index of the items you want to subset. In Python, this sequence of items is called a list.
3. A `list` of columns.

You can see how you can create a list with the names (index IDs) along each of the two
dimensions of a `DataFrame` (rows and columns), and `loc` will return a subset of the
original table only with the elements queried for.

An alternative to list-based queries is what is called “range-based” queries. These work
on the indices of the table, but instead of requiring the ID of each item you want to
retrieve, they operate by requiring only two IDs: the first and last element in a range
of items. Range queries are expressed with a colon (`:`). For example:

```{python}
range_query = chicago_1918.loc[
    "G17003100010":"G17003100012",
    "influenza":'total_population',
]
range_query
```

The range query picks up all the elements between the specified IDs. Note that for this
to work, the first ID in the range needs to be placed before the second one in the table’s index.

Once you know about list and range-based queries, you can combine them! For example, you
can specify a range of rows and a list of columns:

```{python}
range_list_qry = chicago_1918.loc[
    "G17003100010":"G17003100012", ["influenza", "total_population"]
]

range_list_qry
```

### Condition-based queries

However, sometimes, you do not know exactly which observations you want, but you do know
what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For
these cases, `DataFrames` support selection based on conditions. Let us see a few examples.
Suppose you want to select...

_... areas with more than 60 cases of influenza deaths:_

```{python}
flu_over_60 = chicago_1918.loc[chicago_1918["influenza"] > 60]
flu_over_60
```

_... areas with less than 200 inhabitants:_

```{python}
pop_under = chicago_1918.loc[chicago_1918["total_population"] < 200]
pop_under
```

_... areas with exactly a hundred illiterate persons:_

```{python}
illit_100 = chicago_1918.loc[chicago_1918["illit"] == 100]
illit_100
```

::: {.callout-note}
# Unlimited power

These queries can grow in sophistication with almost no limits. For example, here is a
case where you want to find out the areas where the oldest age group is more than half the
population:

```{python}
chicago_1918.loc[
    (chicago_1918["agecat7"] * 100 / chicago_1918["total_population"]) > 50
]
```
:::

All the condition-based queries above are expressed using the `loc` operator. This is a
powerful way, and since it shares syntax with index-based queries, it is also easier to
remember. However, sometimes querying using `loc` involves a lot of quotation marks,
parenthesis, etc. A more streamlined approach for condition-based queries of rows is
provided by the `query` engine. Using this approach, you express everything in our query
on a single string, or piece of text, and that is evaluated in the table at once. For
example, you can run the same operation as in the first query above with the following
syntax:

```{python}
flu_over_60_query = chicago_1918.query("influenza > 60")
flu_over_60_query
```

If you want to combine operations, this is also possible:

```{python}
flu_query = chicago_1918.query("(influenza > 60) & (total_population < 10000)")
flu_query
```

Note that, in these cases, using query results in code that is much more streamlined and
easier to read. However, `query` is not perfect and, particularly for more sophisticated
queries, it does not afford the same degree of flexibility. For example, the last `query`
we had using loc would not be possible using `query`.

::: {.callout-tip}
If you are interested, more detail about `query` is available in the
[pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#the-query-method).
:::

### Combining queries

Now, all of these queries can be combined with each other for further flexibility. For
example, imagine you want areas with more than 60 cases of influenza from areas with less than 10,000 inhabitants:

```{python}
flu_loc = chicago_1918.loc[
    (chicago_1918["influenza"] > 60)
    & (chicago_1918["total_population"] < 10000)  # <1>
]
flu_loc
```
1. The `&` operator combines both conditions together.

::: {.callout-note collapse="true"}
## How do the `loc` queries work?

Let's unpack how these queries work. Each part of the query above creates a single
`Series` with boolean (`True` or `False`) values, encoding whether the row fulfils the condition
or not.

```{python}
chicago_1918["influenza"] > 60
```

```{python}
chicago_1918["total_population"] < 10000
```

You then combine two of these Series with `&`, asking for a new `Series` where values in
both the first and the second `Series` are `True`.

```{python}
(chicago_1918["influenza"] > 60) & (chicago_1918["total_population"] < 10000)
```

Such a `Series` is then essentially used as a mask, and `loc` returns only those columns
that contain `True` in that mask.
:::

### Sorting

Among the many operations `DataFrame` objects support, one of the most useful ones is to
sort a table based on a given column. For example, imagine you want to sort the table by
the influenza cases:

```{python}
chicago_sorted = chicago_1918.sort_values('influenza', ascending=False)  # <1>
chicago_sorted
```
1. By default, `pandas` is sorting from the smallest to the largest values (ascending).
By specifying `ascending=False`, you switch the order.

Given the areas of each census tract differ, it may be better to sort by the mortality rate
rather than raw counts.

```{python}
chicago_1918["flu_rate"] = (                                                # <1>
    chicago_1918["influenza"] / chicago_1918["total_population"]            # <1>
)                                                                           # <1>
chicago_sorted_rel = chicago_1918.sort_values('flu_rate', ascending=False)  # <2>
chicago_sorted_rel
```
1. Compute the relative rate and assign it as a new column.
2. Sort values by this new column.

If you inspect the help of `chicago_1918.sort_values`, you will find that you can pass
more than one column to sort the table by. This allows you to do so-called hierarchical
sorting: sort first based on one column, if equal, then based on another column, etc.

## Visual Exploration

The next step to continue exploring a dataset is to get a feel for what it looks like, visually. You have already learnt how to unconver and inspect specific parts of the data, to check for particular cases you might be interested in. Now, you will see how to plot the data to get a sense of the overall distribution of values. For that, you can use the plotting capabilities of `pandas`.

### Histograms

One of the most common graphical devices to display the distribution of values in a variable is a histogram. Values are assigned into groups of equal intervals, and the groups are plotted as bars rising as high as the number of values into the group.

A histogram is easily created with the following command. In this case, let us have a look at the shape of the overall influenza rates:

```{python}
# | fig-cap: Histogram of influenza cases
_ = chicago_1918["influenza"].plot.hist()
```

::: {.callout-note collapse="true"}
# Assigning to `_`

`pandas` returns an object with the drawing from its plotting methods. Since you are in
Jupyter environment, and you don't need to work further with that object; you can assign
it to `_`, a convention for an unused variable.
:::

However, the default `pandas` plots can be a bit dull. A better option is to use another
package, called [`seaborn`](https://seaborn.pydata.org).

```{python}
import seaborn as sns
```

::: {.callout-note collapse="true"}
# Why `sns`?

`seaborn` is, by convention, imported as `sns`. That came as a joke after
[Samuel Normal Seaborn](https://en.wikipedia.org/wiki/Sam_Seaborn),
a fictional character The West Wing show.
:::

The same plot using `seaborn` has a more pleasant default style and more customisability.

```{python}
# | fig-cap: Histogram of influenza cases using `seaborn`
sns.displot(chicago_1918["influenza"])
```

Note you are using `sns` instead of `pd`, as the function belongs to `seaborn` instead of `pandas`.

 You can quickly see most of the areas have seen somewhere between 0 and 60 cases, approx. However, there are a few areas that have more, up to more than 80 cases.

### Kernel Density Plots

Histograms are useful, but they are artificial in the sense that a continuous variable is made discrete by turning the values into discrete groups. An alternative is kernel density estimation (KDE), which produces an empirical density function:

```{python}
# | fig-cap: Kernel density plot of influenza cases
sns.displot(chicago_1918["influenza"], kind="kde", fill=True)  # <1>
```
1. `kind="kde"` specifies which type of a distribution plot should `seaborn` use and
`fill=True` tells it to colour the area under the KDE curve.

### Line and bar plots

Another very common way of visually displaying a variable is with a line or a bar chart. For example, if you want to generate a line plot of the (sorted) total cases by area:

```{python}
# | fig-cap: Total cases by area (sorted)
_ = chicago_1918["influenza"].sort_values(ascending=False).plot()
```

For a bar plot all you need to do is to change from `plot` to `plot.bar`. Since there are many census tracts, let us plot only the ten largest ones (which you can retrieve with `head`):

```{python}
# | fig-cap: Total cases by area as a bar plot
_ = chicago_1918["influenza"].sort_values(ascending=False).head(10).plot.bar()
```

 You can turn the plot around by displaying the bars horizontally (see how it's just changing `bar` for `barh`). Let's display now the top 50 areas and, to make it more readable, let us expand the plot's height:

```{python}
# | fig-cap: Total cases by area as a horizontal bar plot
_ = (
    chicago_1918["total_population"]
    .sort_values()
    .head(50)
    .plot.barh(figsize=(6, 20))
)
```

::: {.callout-note}
# One line or multiple lines?

You may have noticed that in some cases, the code is on a single line, but longer code
is split into multiple lines. Python requires you to follow the [indentation rules](https://peps.python.org/pep-0008/), but
apart from that, there are not a lot of other limits.
:::

## Tidy data

::: {.callout-caution}
This section is a bit more advanced and hence considered optional. Feel free to skip it, move to the next, and return later when you feel more confident.
:::

Once you can read your data in, explore specific cases, and have a first visual approach to the entire set, the next step can be preparing it for more sophisticated analysis. Maybe you are thinking of modeling it through regression, or on creating subgroups in the dataset with particular characteristics, or maybe you simply need to present summary measures that relate to a slightly different arrangement of the data than you have been presented with.

For all these cases, you first need what statistician, and general R wizard, Hadley Wickham calls *"tidy data"*. The general idea to "tidy" your data is to convert them from whatever structure they were handed in to you into one that allows convenient and standardized manipulation, and that supports directly inputting the data into what he calls "*tidy*" analysis tools. But, at a more practical level, what is exactly *"tidy data"*? In Wickham's own words:

> *Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is
messy or tidy depending on how rows, columns and tables are matched up with observations,
variables and types.*

He then goes on to list the three fundamental characteristics of *"tidy data"*:

1. Each variable forms a column.
1. Each observation forms a row.
1. Each type of observational unit forms a table.

If you are further interested in the concept of *"tidy data"*, I recommend you check out the [original paper](http://www.jstatsoft.org/v59/i10/) (open access) and the [public repository](https://github.com/hadley/tidy-data) associated with it.

Let us bring in the concept of "*tidy data*" to our own Chicago dataset. First, remember its structure:

```{python}
chicago_1918.head()
```

Thinking through *tidy* lenses, this is not a tidy dataset. It is not so for each of the three conditions:

* Starting by the last one (*each type of observational unit forms a table*), this dataset actually contains not one but many observational units: the different areas of Chicago, captured by `geography_code`; *and* different observatoins for each area. To *tidy* up this aspect, you can create separate tables. You will probably want population groups divided by age as one tidy table and flu rates as another.
Start by extracting relevant columns.

```{python}
influenza_rates = chicago_1918[["influenza"]]  # <1>
influenza_rates.head()
```
1. You are not selecting a single columns with `chicago_1918["influenza"]` but a subset
of columns. Just that the subset contains only one column, so you pass a list with a single
column name as `chicago_1918[["influenza"]]`. Notice the double brackets.

```{python}
population = chicago_1918.loc[:, "agecat1":"agecat7"]
population.head()
```

At this point, the table `influenza_rates` is tidy: every row is an observation, every table is a variable, and there is only one observational unit in the table.

The other table (`population`), however, is not entirely tidied up yet: there is only one observational unit in the table, true; but every row is not an observation, and there are variable values as the names of columns (in other words, every column is not a variable). To obtain a fully tidy version of the table, you need to re-arrange it in a way that every row is an age category in an area, and there are three variables: `geography_code`, age category, and population count (or frequency).

Because this is actually a fairly common pattern, there is a direct way to solve it in `pandas`:

```{python}
tidy_population = population.stack()
tidy_population.head()
```

The method `stack`, well, "stacks" the different columns into rows. This fixes our "tidiness" problems but the type of object that is returning is not a `DataFrame`:

```{python}
type(tidy_population)
```

It is a `Series`, which really is like a `DataFrame`, but with only one column. The additional information (`geography_code` and age category) are stored in what is called an multi-index. You will skip these for now, so you would really just want to get a `DataFrame` as you know it out of the `Series`. This is also one line of code away:


```{python}
tidy_population_df = tidy_population.reset_index()
tidy_population_df.head()
```

To which you can apply to renaming to make it look better:

```{python}
tidy_population_df = tidy_population_df.rename(
    columns={"level_1": "age_category", 0: "count"}
)
tidy_population_df.head()
```

Now our table is fully tidied up!

## Grouping, transforming, aggregating

One of the advantage of tidy datasets is they allow to perform advanced transformations in a more direct way. One of the most common ones is what is called "group-by" operations. Originated in the world of databases, these operations allow you to group observations in a table by one of its labels, index, or category, and apply operations on the data group by group.

For example, given our tidy table with age categories, you might want to compute the total sum of the population by each category. This task can be split into two different ones:

* Group the table in each of the different subgroups.
* Compute the sum of `count` for each of them.

To do this in `pandas`, meet one of its workhorses, and also one of the reasons why the library has become so popular: the `groupby` operator.

```{python}
pop_grouped = tidy_population_df.groupby("age_category")
pop_grouped
```

The object `pop_grouped` still hasn't computed anything. It is only a convenient way of specifying the grouping. But this allows us then to perform a multitude of operations on it. For our example, the sum is calculated as follows:

```{python}
pop_grouped.sum(numeric_only=True)  # <1>
```
1. You want a sum of numeric values, not strings. Try it with `numeric_only=False` to see the difference.

Similarly, you can also obtain a summary of each group:

```{python}
pop_grouped.describe()
```

You will not get into it today as it goes beyond the basics this chapter wants to cover, but keep in mind that `groupby` allows you to not only call generic functions (like `sum` or `describe`), but also your own functions. This opens the door for virtually any kind of transformation and aggregation possible.

::: {.callout-tip}
# Additional reading

- A good introduction to data manipulation in Python is Wes McKinney's "[Python for Data Analysis](https://wesmckinney.com/book/pandas-basics)" [@mckinney2012python].
- To explore further some of the visualization capabilities in at your fingertips, the Python library `seaborn` is an excellent choice. Its online [tutorial](https://seaborn.pydata.org/tutorial.html) is a fantastic place to start.
-  A good extension is Hadley Wickham's "Tidy data" paper [@wickham2014tidy], which presents a very popular way of organising tabular data for efficient manipulation.
:::

## Acknowledgements {.appendix}

This section is derived from _A Course on Geographic Data Science_ by
@darribas_gds_course, licensed under CC-BY-SA 4.0. The text was slightly adapted, mostly
to accommodate a different dataset used.