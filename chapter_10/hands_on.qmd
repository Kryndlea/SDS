---
title: Regression and geography
format:
  html: default
  ipynb: default
jupyter: sds
---

::: {.callout-warning}

## The contents is not complete

This section is in works and is not yet complete.

:::

When trying to determine the effect of some (independent) variables on the outcome of phenomena (dependent variable), you often
use regression to model such an outcome and understand the influence each of the variables has in the model.
With spatial regression, it is the same. You just need to use the spatial dimension in a
mindful way.

This chapter provides an introduction to ways of incorporating space into regression models, from
spatial variables in standard linear regression to geographically weighted regression.

```{python}
import geopandas as gpd
import pandas as pd
import numpy as np
import mgwr
import seaborn as sns
import matplotlib.pyplot as plt
from libpysal import graph
import statsmodels.formula.api as sm
import esda
from splot.esda import lisa_cluster
```

## Data

You will work with the same data you already used in the chapter on [spatial autocorrelation](../chapter_05/hands_on.qmd) - the results of the second round of the presidential elections in Czechia in 2023, between Petr Pavel and Andrej Babi≈°, on a level of municipalities. You can read the election data directly from the chapter 5 location.

```{python}
elections = gpd.read_file(
    "https://martinfleischmann.net/sds/chapter_05/data/cz_elections_2023.gpkg"
)
elections = elections.set_index("name")
elections.head()
```

::: {.callout-note collapse="true"}
## Alternative
Instead of reading the file directly off the web, it is possible to download it manually,
store it on your computer, and read it locally. To do that, you can follow these steps:

1. Download the file by right-clicking on
[this link](https://martinfleischmann.net/sds/chapter_05/data/cz_elections_2023.gpkg)
and saving the file
2. Place the file in the same folder as the notebook where you intend to read it
3. Replace the code in the cell above with:

```python
elections = gpd.read_file(
    "cz_elections_2023.gpkg",
)
```
:::

The election results give you the dependent variable - you will look at the percentage of votes Petr Pavel, the winner, received. From the [map of the results](../chapter_05/hands_on.qmd#code-cell-2) and the analysis you did when exploring spatial autocorrelation you already know that there are some significant spatial patterns. Let's look whether these patterns correspond to the composition of education levels within each municipality.

You can use the data from the [Czech Statistical Office](https://www.czso.cz/csu/czso/vysledky-scitani-2021-otevrena-data) reflecting the situation during the Census 2021. The original table has been [preprocessed](../data/cz_education_2021/preprocessing.ipynb) and is available as a CSV.

```{python}
education = pd.read_csv(
    "https://martinfleischmann.net/sds/chapter_10/data/education.csv"
)
education.head()
```

::: {.callout-note collapse="true"}
## Alternative
Instead of reading the file directly off the web, it is possible to download it manually,
store it on your computer, and read it locally. To do that, you can follow these steps:

1. Download the file by right-clicking on
[this link]("https://martinfleischmann.net/sds/chapter_10/data/education.csv")
and saving the file
2. Place the file in the same folder as the notebook where you intend to read it
3. Replace the code in the cell above with:

```python
education = pd.read_csv(
    "education.csv",
)
```
:::

The first thing you need to do is to merge the two tables, to have both dependent and independent variables together. The municipality code in the `elections` table is in the `"nationalCode"` column, while in the education table in the `"uzemi_kod"` column.

```{python}
elections_data = elections.merge(education, left_on="nationalCode", right_on="uzemi_kod")
elections_data.head()
```

That is all sorted and ready to be used in a regression.

## Non-spatial linear regression

Before jumping into spatial regression, let's start with the standard linear regression. A useful start is to explore the data using an ordinary least squares (OLS) linear regression model.

### OLS model

While this course is not formula-heavy, in this case, it is useful to use the formula to explain the logic of the algorithm. The OLS tries to model the dependent variable $y$ as the linear combination of independent variables $x_1, x_2, ... x_n$:

$$y_{i}=\alpha+\beta _{1}\ x_{i1}+\beta _{2}\ x_{i2}+\cdots +\beta _{p}\ x_{ip}+\varepsilon _{i}$$

where $\epsilon_{i}$ represents unobserved random variables and $\alpha$ represents an intercept - a constant. You know the $y_i$, all of the $x_i$ and try to estimate the coefficients. In Python, you can run linear regression using implementations from more than one package (e.g., `statsmodels`, `scikit-learn`, `spreg`). This course covers `statsmodels` approach as it has a nice API to work with.

First, you need a list of names of independent variables. That is equal to column names without a few of the columns that represent other data.

```{python}
independent_names = education.columns.drop(["uzemi_kod", "okres", "without_education"])
independent_names
```

`statsmodels` (above imported as `sm`) offers an intuitive formula API to define the linear regression.

```{python}
formula = f"PetrPavel ~ {' + '.join(independent_names)}"  # <1>
formula
```
1. In the formula, specify the dependent variable (`"PetrPavel"`) as a function of (`"~"`) independent variables (`"undetermined + incomplete_primary_education + ..."`).

With the formula ready, you can fit the model and estimate all betas and $\varepsilon$.

```{python}
ols = sm.ols(formula, data=elections_data).fit()
```

The `ols` object offers a handy `summary()` function providing most of the results from the fitting in one place.

```{python}
#| classes: explore
ols.summary()
```

It is clear that education composition has a significant effect on the outcome of the elections but can explain only about 42% of its variance (adjusted $R^2$ is 0.422). A higher amount of residents with only primary education tends to lower Pavel's gain while a higher amount of university degrees tends to increase the number of votes he received. That is nothing unexpected. However, let's make use of geography and unpack these results a bit.


### Spatial exploration of the model (hidden structures)

Start with the visualisation of the prediction the OLS model produces using the coefficients shown above.

```{python}
predicted = ols.predict(elections_data)  # <1>
predicted.head()
```
1. Use the `predict()` method with the original data to get the prediction using the model.

Make a plot comparing the prediction with the actual results.

```{python}
# | fig-cap: OLS prediction and the actual outcome
f, axs = plt.subplots(2, 1, figsize=(7, 8)) # <1>
elections_data.plot(    # <2>
    predicted, legend=True, cmap="coolwarm", vmin=0, vmax=100, ax=axs[0]    # <2>
)    # <2>
elections_data.plot(    # <3>
    "PetrPavel", legend=True, cmap="coolwarm", vmin=0, vmax=100, ax=axs[1]    # <3>
)    # <3>
axs[0].set_title("OLS prediction") # <4>
axs[1].set_title("Actual results") # <4>

axs[0].set_axis_off() # <5>
axs[1].set_axis_off() # <5>
```
1. Create a subplot with two axes.
2. Plot the `predicted` data on the `elections_data` geometry.
3. Plot the original results.
4. Set titles for axes in the subplot.
5. Remove axes borders.

The general patterns are captured but there are some areas of the country which seem to be quite off. The actual error between prediction and the dependent variable is captured as _residuals_, which are directly available in `ols` as `ols.resid` attribute. Let's plot to get a better comparison.

```{python}
# | fig-cap: Residuals of the OLS prediction
elections_data["residual"] = ols.resid  # <1>
max_residual = ols.resid.abs().max()  # <2>
ax = elections_data.plot( # <3>
    "residual", legend=True, cmap="RdBu", vmin=-max_residual, vmax=max_residual # <3>
)  # <3>
ax.set_axis_off()
```
1. Assign residuals as a column. This is not needed for the plot but it will be useful later.
2. Identify the maximum residual value based on absolute value to specify `vmin` and `vmax` values of the colormap.
3. Plot the data using diverging colormap centred around 0.

All of the municipalities in blue (residual above 0) have reported higher gains for Petr Pavel than the model assumes based on education structure, while all in red reported lower gains than what is expected. However, as data scientists, we have better tools to analyse the spatial structure of residuals than eyeballing it. Let's recall the chapter on spatial autocorrelation again and figure out the spatial clusters of residuals.

First, create a contiguity graph and row-normalise it.

```{python}
contiguity_r = graph.Graph.build_contiguity(elections_data).transform("r")
```

Then you can generate a Moran plot of residuals. For that, you will need the lag of residuals.

```{python}
elections_data["residual_lag"] = contiguity_r.lag(elections_data["residual"])
```

And then you can use the code from the earlier chapter to generate a Moran scatterplot using `seaborn`.

```{python}
#| fig-cap: Moran Plot
f, ax = plt.subplots(1, figsize=(6, 6))
sns.regplot(
    x="residual",
    y="residual_lag",
    data=elections_data,
    marker=".",
    scatter_kws={"alpha": 0.2},
    line_kws=dict(color="lightcoral")
)
plt.axvline(0, c="black", alpha=0.5)
plt.axhline(0, c="black", alpha=0.5);
```

That looks like a pretty strong relationship. Use the local version of Moran's statistic to find out the clusters.

```{python}
lisa = esda.Moran_Local(elections_data['residual'], contiguity_r.to_W())  # <1>
```
1. Use `Moran_Local` function from `esda` and remember that it needs `W`, not `Graph`.

Let's use our handy `lisa_cluster` function from `splot` to visualise the results.

```{python}
#| fig-cap: LISA clusters
_ = lisa_cluster(lisa, elections_data)
```

The outcome of LISA shows large clusters of both overpredicted (high-high) and underpredicted (low-low) areas. The underpredicted are mostly in central Bohemia around Prague and in the mountains near the borders, where the ski resorts are. Putting aside the central areas for a bit, the explanation of underprediction in mountains is relatively straightforward. The education data are linked to the residents of each municipality. The people who voted in a municipality do not necessarily need to match with residents. It is known that more affluent population groups, who are more likely to go to a ski resort, voted overwhelmingly for Pavel. And since the elections were in winter, a lot of them likely voted in ski resorts, affecting the outcome of the model.

The overpredicted areas, on the other hand, are known for higher levels of deprivation, which may have played a role in the results. What is clear, is that geography plays a huge role in the modelling of the elections.

## Spatial heterogeneity

Not all areas behave equally, it seems that some systematically vote for Pavel more than for Babi≈° while others vote for him less. You need to account for this when building a regression model. One way is by capturing _spatial heterogeneity_. It implicitly assumes that the outcome of the model spatially varies. You can expect $\alpha$ to vary across space, or individual values of $\beta$. Spatial fixed effects capture the former.

### Spatial fixed effects

You need to find a way to let $\alpha$ change across space. One option is through the proxy variable capturing higher-level geography. You have information about _okres_ (the closest translation to English would probably be district or county) each municipality belongs to. Let's start by checking if that could be useful by visualising residuals within each. While you can use the box plot directly, it may be better to sort the values by median residuals, so let's complicate the code a bit.

```{python}
medians = ( # <1>
    elections_data.groupby("okres") # <1>
    .residual.median() # <1>
    .to_frame("okres_residual") # <1>
) # <1>
f, ax = plt.subplots(figsize=(16, 6))
sns.boxplot( # <2>
    data=elections_data.merge( # <3>
        medians, how="left", left_on="okres", right_index=True # <3>
    ).sort_values("okres_residual"), # <4>
    x="okres", # <5>
    y="residual", # <6>
)
_ = plt.xticks(rotation=90) # <7>
```
1. Get median residual value per _okres_ using `groupby` and convert the resulting `Series` to `DataFrame` to be able to merge it with the original data.
2. Create a box plot and pass the data.
3. The data is the `elections_data` table merged with the `medians` that are after merge stored as the `"okres_residual"` column.
4. Sort by the `"okres_residual"` column.
5. The x value should represent each _okres_.
6. The y value should represent residuals.
7. Rotate x tick labels by 90 degrees for readability.


```{python}
formula_fe = f"PetrPavel ~ {' + '.join(independent_names)} + okres - 1"
ols_fe = sm.ols(formula_fe, data=elections_data).fit()
```

```{python}
ols_fe.summary()
```

```{python}
fixed_effects = ols_fe.params.filter(like="okres")
fixed_effects.head()
```

```{python}
fixed_effects.index = fixed_effects.index.str.strip(
    "okres["
).str.strip("]")
fixed_effects.head()
```

```{python}
elections_data.merge(fixed_effects.to_frame("fixed_effect"), left_on="okres", right_index=True, how="left").plot("fixed_effect", legend=True, vmin=30, vmax=70, cmap="PRGn").set_axis_off()
```


::: {.callout-tip}
# Spatial regimes and spatial dependence

Link to the book

### Spatial regimes

_differernces of constant and coefficients - spreg_

## Spatial dependence (possibly only via literature?)

_based on spatial configuration via W_
    - _lagged explanatory_
    - _lagged error_
    - _lagged dependent as another explanatory (no OLS)_
:::

## Geographically weighted regression

- kernels
    - shape
    - size
    - fixed / adaptive

https://smds-book.github.io/smds/08-weighted-regression-modelling.html

### Fixed bandwidth

```{python}
coords = elections_data.centroid.get_coordinates().values
y = elections_data[f"PetrPavel"].values.reshape(-1, 1)
X = elections_data[independent_names].values
```

```{python}
fixed_bandwidth = mgwr.gwr.GWR(coords, y, X, bw=25_000, fixed=True, name_x=independent_names)
results_fb = fixed_bandwidth.fit()
```

```{python}
results_fb.summary()
```

```{python}
elections_data.plot(results_fb.localR2.flatten(), legend=True).set_axis_off()
```

### Adaptive bandwidth

```{python}
sel_bw = mgwr.sel_bw.Sel_BW(coords, y, X)
bw = sel_bw.search()
bw
```

```{python}
adaptive = mgwr.gwr.GWR(coords, y, X, bw=bw, fixed=False, name_x=independent_names)
results_ab = adaptive.fit()
```

```{python}
results_ab.summary()
```

```{python}
elections_data.plot(results_ab.localR2.flatten(), legend=True).set_axis_off()
```

```{python}
f, axs = plt.subplots(3, 1, figsize=(7, 14))
elections_data.plot(ols_fe.predict(elections_data), legend=True, cmap="coolwarm",
 ax=axs[0])
elections_data.plot(results_ab.predy.flatten(), legend=True, cmap="coolwarm",
    vmin=0,
    vmax=100, ax=axs[1]).set_axis_off()

elections_data.plot("PetrPavel", legend=True, cmap="coolwarm",
   vmin=0,
    vmax=100, ax=axs[2])
axs[0].set_title("OLS prediction")
axs[1].set_title("GWR prediction")
axs[2].set_title("Actual results")

axs[0].set_axis_off()
axs[1].set_axis_off()
axs[2].set_axis_off()
```

```{python}
f, axs = plt.subplots(2, 1, figsize=(7, 8))
elections_data.plot("residual", legend=True, cmap="RdBu", vmin=-max_residual, vmax=max_residual, ax=axs[0])
elections_data.plot(results_ab.resid_response.flatten(), legend=True, cmap="RdBu", vmin=-max_residual, vmax=max_residual, ax=axs[1])
axs[0].set_title("OLS residuals")
axs[1].set_title("GWR residuals")

axs[0].set_axis_off()
axs[1].set_axis_off()
```

```{python}
results_ab.adj_alpha
```

```{python}
sig95 = results_ab.adj_alpha[1]
critical_t = results_ab.critical_tval(alpha=sig95)
critical_t
```

```{python}
significant = np.abs(results_ab.tvalues) > critical_t

fig, axs = plt.subplots(4, 3, figsize=(9, 9))
axs = axs.flatten()
for i, name in enumerate(independent_names[1:]):
    significant_mask = significant[:, i + 2]
    elections_data.plot(results_ab.params[:, i + 2], cmap="plasma", ax=axs[i])
    elections_data[~significant_mask].plot(color="white", ax=axs[i], alpha=.9)
    axs[i].set_title(name[:20], fontdict={'fontsize': 8})
    axs[i].set_axis_off()
```


