---
title: "Similar data belong together"
format:
    revealjs:
        theme: [default, ../assets/reveal.scss]
        logo: ../assets/logo.svg
        menu: false
        transition: fade
        navigation-mode: linear
        controls-layout: edges
jupyter: sds
---

> Everything should be made as simple as possible, but not simpler

Albert Einstein


## The need to group data

---


::: {.r-fit-text}
The world is **complex** and **multidimensional**
:::

::: {.r-fit-text .fragment .fade-in}
**Univariate** analysis focuses on **only one** dimension
:::

::: {.r-fit-text .fragment .fade-in}
Sometimes, world issues are best understood as **multivariate**
:::

---

### For example

::: {.fragment .fade-in}
#### Percentage of foreign-born vs *What is a neighborhood?*
:::

::: {.fragment .fade-in}
#### Years of schooling vs *Human development*
:::

::: {.fragment .fade-in}
#### Monthly income vs *Deprivation*
:::

## Grouping as simplifying

::: {.r-fit-text .fragment .fade-in}
Define a given number of categories based on <br>
**many characteristics** (multi-dimensional)
:::

::: {.r-fit-text .fragment .fade-in}
Find the **category** where each observation *fits best*.
:::

::: {.r-fit-text .fragment .fade-in}
**Reduce complexity**, keep all the **relevant information**
:::

::: {.r-fit-text .fragment .fade-in}
Produce easier-to-understand outputs
:::

## Types of grouping

#### Non-spatial clustering{.fragment .fade-in}
#### Regionalisation{.fragment .fade-in}

# Non-spatial clustering

::: {.r-fit-text}

[**Split** a dataset into **groups** of observations]{.fragment .fade-in}
[that are **similar within** the group]{.fragment .fade-in}<br>
[and **dissimilar between** groups]{.fragment .fade-in}
[based on a series of **attributes**]{.fragment .fade-in}

:::

## Machine learning

The computer *learns* some of the dataset's properties without the human specifying them.

### Unsupervised{.fragment .fade-in}

::: {.fragment .fade-in}
  There is no apriori structure imposed on the classification $\rightarrow$
  before the analysis, no observations are in a category.
:::

## K-means

#### Most popular clustering algorithm{.fragment .fade-in}
#### Good but not perfect{.fragment .fade-in}

---

> partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid)

Wikipedia

## Example

### Palmer Penguins{.fragment .fade-in}

```{python}
#| echo: false
import time
import warnings
from itertools import cycle, islice

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import cluster, datasets, mixture

import numpy as np

from sklearn import cluster, datasets, mixture
from sklearn.neighbors import kneighbors_graph
from sklearn.preprocessing import StandardScaler

from matplotlib import cm
from matplotlib.colors import rgb2hex

sns.set(style="white")
penguins = sns.load_dataset("penguins").dropna()

kmeans = cluster.KMeans(3).fit(penguins[["bill_length_mm", "flipper_length_mm"]])
penguins['Cluster (no stand.)'] = kmeans.labels_

df = penguins[["bill_length_mm", "flipper_length_mm"]]
stand = (df - df.mean() )/ df.std()

kmeans_std = cluster.KMeans(3).fit(stand)
penguins['Cluster'] = kmeans_std.labels_
```

---

![The Palmer Archipelago penguins. Artwork by @allison_horst](figures/lter_penguins.png)

---

```{python}
# | fig-cap: Penguins according to their flipper length and bill length
sns.scatterplot(data=penguins, x="bill_length_mm", y="flipper_length_mm", hue="species", style="species", palette="Set2")
sns.despine(offset=10)
plt.legend(frameon=False);
```

---

```{python}
# | fig-cap: K-Means clusters based on flipper length and bill length
ax= sns.scatterplot(data=penguins, x="bill_length_mm", y="flipper_length_mm", hue="Cluster", style="species", palette="Set2", legend=True)
sns.despine(offset=10)
plt.legend(frameon=False)
plt.scatter(*((kmeans_std.cluster_centers_ * df.std().values) + df.mean().values).T, color="k");
```

---

::: {.r-fit-text .absolute top=25%}

distance-based

:::

---

```{python}
# | fig-cap: K-Means clusters based on non-standardised data
ax= sns.scatterplot(data=penguins, x="bill_length_mm", y="flipper_length_mm", hue="Cluster (no stand.)", style="species", palette="Set2", legend=True)
sns.despine(offset=10)
plt.legend(frameon=False)
plt.scatter(*kmeans.cluster_centers_.T, color="k");
```

---

```{python}
# | fig-cap: Pay attention to scales
ax= sns.scatterplot(data=penguins, x="bill_length_mm", y="flipper_length_mm", hue="Cluster (no stand.)", style="species", palette="Set2", legend=False)
sns.despine(offset=10)
plt.scatter(*kmeans.cluster_centers_.T, color="k")
ax.set_aspect("equal");
```

---

::: {.r-fit-text .absolute top=25%}

always standardise

:::

## Many techniques

#### Hierarchical clustering{.fragment .fade-in}
#### Agglomerative clustering{.fragment .fade-in}
#### Spectral clustering{.fragment .fade-in}
#### Neural networks (e.g. Self-Organizing Maps){.fragment .fade-in}
#### DBSCAN{.fragment .fade-in}

---

```{python}
# | fig-cap: Comparison of algorithms

# https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html
# ============
# Generate datasets. We choose the size big enough to see the scalability
# of the algorithms, but not too big to avoid too long running times
# ============
n_samples = 500
seed = 30
noisy_circles = datasets.make_circles(
    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed
)
noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)
blobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)
rng = np.random.RandomState(seed)
no_structure = rng.rand(n_samples, 2), None

# Anisotropicly distributed data
random_state = 170
X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)
transformation = [[0.6, -0.6], [-0.4, 0.8]]
X_aniso = np.dot(X, transformation)
aniso = (X_aniso, y)

# blobs with varied variances
varied = datasets.make_blobs(
    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state
)

# ============
# Set up cluster parameters
# ============
plt.figure(figsize=((9 * 2 + 3) /2 , 13/2))
plt.subplots_adjust(
    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01
)

plot_num = 1

default_base = {
    "quantile": 0.3,
    "eps": 0.3,
    "damping": 0.9,
    "preference": -200,
    "n_neighbors": 3,
    "n_clusters": 3,
    "min_samples": 7,
    "xi": 0.05,
    "min_cluster_size": 0.1,
    "allow_single_cluster": True,
    "hdbscan_min_cluster_size": 15,
    "hdbscan_min_samples": 3,
    "random_state": 42,
}

datasets = [
    (
        noisy_circles,
        {
            "damping": 0.77,
            "preference": -240,
            "quantile": 0.2,
            "n_clusters": 2,
            "min_samples": 7,
            "xi": 0.08,
        },
    ),
    (
        noisy_moons,
        {
            "damping": 0.75,
            "preference": -220,
            "n_clusters": 2,
            "min_samples": 7,
            "xi": 0.1,
        },
    ),
    (
        varied,
        {
            "eps": 0.18,
            "n_neighbors": 2,
            "min_samples": 7,
            "xi": 0.01,
            "min_cluster_size": 0.2,
        },
    ),
    (
        aniso,
        {
            "eps": 0.15,
            "n_neighbors": 2,
            "min_samples": 7,
            "xi": 0.1,
            "min_cluster_size": 0.2,
        },
    ),
    (blobs, {"min_samples": 7, "xi": 0.1, "min_cluster_size": 0.2}),
    (no_structure, {}),
]

for i_dataset, (dataset, algo_params) in enumerate(datasets):
    # update parameters with dataset-specific values
    params = default_base.copy()
    params.update(algo_params)

    X, y = dataset

    # normalize dataset for easier parameter selection
    X = StandardScaler().fit_transform(X)

    # estimate bandwidth for mean shift
    bandwidth = cluster.estimate_bandwidth(X, quantile=params["quantile"])

    # connectivity matrix for structured Ward
    connectivity = kneighbors_graph(
        X, n_neighbors=params["n_neighbors"], include_self=False
    )
    # make connectivity symmetric
    connectivity = 0.5 * (connectivity + connectivity.T)

    # ============
    # Create cluster objects
    # ============
    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)
    two_means = cluster.MiniBatchKMeans(
        n_clusters=params["n_clusters"],
        n_init="auto",
        random_state=params["random_state"],
    )
    ward = cluster.AgglomerativeClustering(
        n_clusters=params["n_clusters"], linkage="ward", connectivity=connectivity
    )
    spectral = cluster.SpectralClustering(
        n_clusters=params["n_clusters"],
        eigen_solver="arpack",
        affinity="nearest_neighbors",
        random_state=params["random_state"],
    )
    dbscan = cluster.DBSCAN(eps=params["eps"])
    hdbscan = cluster.HDBSCAN(
        min_samples=params["hdbscan_min_samples"],
        min_cluster_size=params["hdbscan_min_cluster_size"],
        allow_single_cluster=params["allow_single_cluster"],
    )
    optics = cluster.OPTICS(
        min_samples=params["min_samples"],
        xi=params["xi"],
        min_cluster_size=params["min_cluster_size"],
    )
    affinity_propagation = cluster.AffinityPropagation(
        damping=params["damping"],
        preference=params["preference"],
        random_state=params["random_state"],
    )
    average_linkage = cluster.AgglomerativeClustering(
        linkage="average",
        metric="cityblock",
        n_clusters=params["n_clusters"],
        connectivity=connectivity,
    )
    birch = cluster.Birch(n_clusters=params["n_clusters"])
    gmm = mixture.GaussianMixture(
        n_components=params["n_clusters"],
        covariance_type="full",
        random_state=params["random_state"],
    )

    clustering_algorithms = (
        ("KMeans", two_means),
        ("Affinity\nPropagation", affinity_propagation),
        ("MeanShift", ms),
        ("Spectral\nClustering", spectral),
        ("Ward", ward),
        ("Agglomerative\nClustering", average_linkage),
        ("DBSCAN", dbscan),
        ("HDBSCAN", hdbscan),
        ("OPTICS", optics),
        ("BIRCH", birch),
        ("Gaussian\nMixture", gmm),
    )

    for name, algorithm in clustering_algorithms:
        t0 = time.time()

        # catch warnings related to kneighbors_graph
        with warnings.catch_warnings():
            warnings.filterwarnings(
                "ignore",
                message="the number of connected components of the "
                + "connectivity matrix is [0-9]{1,2}"
                + " > 1. Completing it to avoid stopping the tree early.",
                category=UserWarning,
            )
            warnings.filterwarnings(
                "ignore",
                message="Graph is not fully connected, spectral embedding"
                + " may not work as expected.",
                category=UserWarning,
            )
            algorithm.fit(X)

        t1 = time.time()
        if hasattr(algorithm, "labels_"):
            y_pred = algorithm.labels_.astype(int)
        else:
            y_pred = algorithm.predict(X)

        ax = plt.subplot(len(datasets), len(clustering_algorithms), plot_num)
        if i_dataset == 0:
            plt.title(name, size=9)

        colors = np.array(
            list(
                islice(
                    cycle(
                        [rgb2hex(c) for c in cm.Set2.colors]
                    ),
                    int(max(y_pred) + 1),
                )
            )
        )
        # add black color for outliers (if any)
        colors = np.append(colors, ["#000000"])
        plt.scatter(X[:, 0], X[:, 1], s=1, color=colors[y_pred])

        plt.xlim(-2.5, 2.5)
        plt.ylim(-2.5, 2.5)
        plt.xticks(())
        plt.yticks(())
        plot_num += 1
        sns.despine(ax=ax, left=True, bottom=True)

plt.show()
```

# Regionalization

::: {.r-fit-text}

[**Split** a dataset into **groups** of observations]{.fragment .fade-in}
[that are **similar within** the group]{.fragment .fade-in}<br>
[and **dissimilar between** groups]{.fragment .fade-in}
[based on a series of **attributes** ]{.fragment .fade-in}<br>
[with the additional constraint that observations need to be
**spatial neighbours**]{.fragment .fade-in}
:::

---

::: {.r-fit-text .absolute top=40%}

Aggregating basic spatial units (**areas**) into larger units (**regions**)

:::

---

##### All the methods aggregate geographical areas into a predefined number of regions while optimizing a particular aggregation criterion{.fragment .fade-in-then-out}
##### The areas within a region must be geographically connected (the spatial contiguity constraint){.fragment .fade-in-then-out}
##### The number of regions must be smaller than or equal to the number of areas{.fragment .fade-in-then-out}
##### Each area must be assigned to one and only one region{.fragment .fade-in-then-out}
##### Each region must contain at least one area{.fragment .fade-in-then-out}

---

##### All the methods aggregate geographical areas into a predefined number of regions while optimizing a particular aggregation criterion
##### The areas within a region must be geographically connected (the spatial contiguity constraint)
##### The number of regions must be smaller than or equal to the number of areas
##### Each area must be assigned to one and only one region
##### Each region must contain at least one area

Duque et al. (2007)

## Algorithms

#### Automated Zoning Procedure (AZP){.fragment .fade-in}
#### Arisel{.fragment .fade-in}
#### Max-P{.fragment .fade-in}
#### Skater{.fragment .fade-in}

[See [Duque et al. (2007)](http://irx.sagepub.com/content/30/3/195) for an
excellent, though advanced, overview.]{.fragment .fade-in}

---

::: {.r-fit-text .absolute top=25%}
import sklearn
:::